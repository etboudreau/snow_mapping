{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In this notebook:\n",
    "- Systematically remove scenes:  >2% unobserved pixels\n",
    "- Manually remove scenes: visual inspection of duplicate dates\n",
    "- Create a csv with PS filenames for corresponding ASO filenames\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as colors\n",
    "import geopandas as gpd\n",
    "import seaborn as sns\n",
    "from matplotlib.ticker import ScalarFormatter\n",
    "import rioxarray as rxr\n",
    "from datetime import datetime\n",
    "import glob\n",
    "import os\n",
    "import contextily as ctx\n",
    "import xarray as xr\n",
    "from collections import Counter\n",
    "import cmcrameri.cm as cmc\n",
    "import shutil\n",
    "\n",
    "from working_sca_funcs import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define year you are working on\n",
    "year = '2022'\n",
    "\n",
    "# Define basin you are working on\n",
    "# name = 'DLNY'\n",
    "# name = 'LYMC'\n",
    "# name = 'LYTB'\n",
    "# name = 'H120'\n",
    "# name = 'DNBC'\n",
    "# name = 'BUDD'\n",
    "name = 'COPP'\n",
    "\n",
    "# model = 'V1' # Justin's manual classification 2023 DLNY\n",
    "# model = 'V2' # 2023 DLNY ASO 2 class\n",
    "# model = 'V3' # 2023 DLNY ASO 4 class\n",
    "# model = 'V4' # 2023 DLNY ASO 2 class NDVI\n",
    "model = 'base' # 2022 BUDD ASO 2 class\n",
    "# model = 'V6' # 2022 BUDD manual classification snow/no snow/artifact all trained on same images\n",
    "#model = 'V7' # 2022 BUDD manual classification snow/no snow trained on set of images and artifacts on different images\n",
    "# model = 'V9'\n",
    "# raw PS images\n",
    "ps_raw = f'/data0/images/planet/emma/planet/{name}/'\n",
    "ps_raw_subdir = sorted([d for d in glob.glob(ps_raw + str(year) + '*') if os.path.isdir(d)])\n",
    "\n",
    "# SCA directory\n",
    "#ps_sca_dir = f'/data0/images/planet/emma/planet/processed_SCA/{name}/{model}/'\n",
    "ps_sca_dir = f'/home/etboud/projects/data/temp/{name}/{model}/'\n",
    "ps_sca_tif = glob.glob(ps_sca_dir + f'*{year}*.tif')\n",
    "\n",
    "# ASO directory\n",
    "aso_dir = f'/home/etboud/projects/data/aso/validation/{name}/V2/'\n",
    "aso_tif = glob.glob(os.path.join(aso_dir, f'*{year}*3m*.tif')) \n",
    "\n",
    "# Create basin shapefile\n",
    "BS = f'/home/etboud/projects/data/basins/{name}/{name}_4326.geojson'\n",
    "basin = gpd.read_file(BS)\n",
    "basin = basin.to_crs('EPSG:32611') \n",
    "\n",
    "# Extract dates from file names\n",
    "ps_dates = extract_dates([], ps_sca_tif)[1]  \n",
    "aso_dates = extract_dates(aso_tif, [])[0]\n",
    "\n",
    "# canopy height model\n",
    "CHM = '/home/etboud/projects/data/CHM/USCATB20140827_chm_3p0m.tif'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Systematically remove bad scenes\n",
    "Anything with unobs >.02 will be removed\n",
    "Run this when the model is set to V1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chm_mask,mean,max,fcan = create_binary_chm(CHM, basin)\n",
    "df = create_ps_df(ps_sca_tif, basin, name, chm_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize processed maps\n",
    "for file in ps_sca_tif:\n",
    "    if os.path.basename(file).split(\".\")[0] not in df['fn'].values:\n",
    "        sca = rxr.open_rasterio(file, all_touched=False, drop=True, masked=True)\n",
    "        sca.values = np.where(np.isnan(sca.values), 0, sca.values)\n",
    "        sca = sca.rio.clip(basin.geometry.values, crs=basin.crs, drop=True)\n",
    "        sca = np.squeeze(sca.values)\n",
    "        im = plt.imshow(sca,vmin= 0,vmax = 2, interpolation= 'none')\n",
    "        plt.title(os.path.basename(file))\n",
    "        plt.colorbar(im)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Moving files of poor quality to a different directory ( i beleive over 2% unobds)\n",
    "path = f'/home/etboud/projects/data/planet/processed_SCA/{name}/{model}/'\n",
    "move_to = f'/home/etboud/projects/data/planet/processed_SCA/{name}/{model}/not_good/'\n",
    "for file in ps_sca_tif:\n",
    "    if os.path.basename(file).split(\".\")[0] not in df['fn'].values:\n",
    "        src = path+os.path.basename(file)\n",
    "        dst = move_to+os.path.basename(file)\n",
    "        shutil.move(src, dst)\n",
    "        #print(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manually identify which scene are bad\n",
    "Create a not_good folder for one model run and move bad scenes into it after running INSPECTION CELL below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSPECTION CELL\n",
    "# Run on one model directory to manually determine which file to move dor duplicate or triplicate dates\n",
    "# Change len(files_for_date) between 2 or 3 to and axis to 2 or 3\n",
    "\n",
    "ps_dates = extract_dates([], ps_sca_tif)[1]\n",
    "# Extract dates from the list\n",
    "dates = [item[0] for item in ps_dates]\n",
    "\n",
    "# Count occurrences of each date\n",
    "date_counter = Counter(dates)\n",
    "\n",
    "# Find dates that occur more than once\n",
    "duplicate_dates = [date for date, count in date_counter.items() if count > 1]\n",
    "\n",
    "# Plot PS for each duplicate date\n",
    "for date in duplicate_dates:\n",
    "    print(f\"Plotting data for date: {date}\")\n",
    "    files_for_date = [file_path for ps_date, file_path in ps_dates if ps_date == date]\n",
    "    \n",
    "    # Ensure there are exactly two files for this date\n",
    "    if len(files_for_date) == 2:\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(12, 6))  # Create two side-by-side plots\n",
    "        \n",
    "        for i, file_path in enumerate(files_for_date):\n",
    "            fn = os.path.basename(file_path)\n",
    "            # Load the data using rioxarray\n",
    "            data = rxr.open_rasterio(file_path, all_touched=False, drop=True, masked=True)\n",
    "            data.values = np.where(np.isnan(data.values), 0, data.values)\n",
    "            data = data.rio.clip(basin.geometry.values, basin.crs,drop=True)\n",
    "            # Plotting the first band\n",
    "            data.isel(band=0).plot(ax=axes[i], cmap=cmc.batlow)\n",
    "            axes[i].set_title(f\"File: {fn}\")\n",
    "            axes[i].axis('off')  # Turn off axis\n",
    "        \n",
    "        plt.suptitle(f\"Date: {date}\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run for each Basin to create a list of PS scenes that are bad\n",
    "# Run this when model is set to V1 (unobs scenes already in not_good model)\n",
    "dir =  glob.glob(f'/data0/images/planet/emma/planet/processed_SCA/{name}/V5/not_good/'+'*.tif')\n",
    "save_bad = []\n",
    "for file in dir:\n",
    "    if os.path.basename(file)[:4]==year:\n",
    "        fn = os.path.basename(file)\n",
    "        save_bad.append(fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 'not_good' scenes\n",
    "model = 'V5'\n",
    "not_good = f'/data0/images/planet/emma/planet/processed_SCA/{name}/{model}/'\n",
    "dir = glob.glob(not_good + '/*.tif')\n",
    "for file in dir:\n",
    "    fn = os.path.basename(file)\n",
    "    date = fn.split('_')[0]\n",
    "    id = fn.split('_')[1]\n",
    "    code = f'{date}_{id}'\n",
    "    \n",
    "\n",
    "    \n",
    "    for raw_file in ps_raw_subdir:\n",
    "        if code in raw_file:\n",
    "            matching_raw_file = glob.glob(raw_file+'/*/PSScene/*_SR_clip.tif')[0]\n",
    "            rgb_image = rxr.open_rasterio(matching_raw_file,masked = True, drop=True)\n",
    "            rgb_image = rgb_image.rio.clip(basin.geometry, basin.crs)\n",
    "            _, _, _, _, rgb_image = calc_rgb(rgb_image)\n",
    "            \n",
    "            sca = rxr.open_rasterio(file, all_touched=False, drop=True, masked=True)\n",
    "            sca.values = np.where(np.isnan(sca.values), 0, sca.values)\n",
    "            sca = sca.rio.clip(basin.geometry.values, crs=basin.crs, drop=True)\n",
    "            sca = np.squeeze(sca.values)\n",
    "            \n",
    "            plt.subplot(1,2,1)\n",
    "            plt.imshow(sca,vmin= 0,vmax = 2, interpolation= 'none')\n",
    "            plt.colorbar()\n",
    "            plt.title(code)\n",
    "            plt.axis('off')\n",
    "            \n",
    "            plt.subplot(1,2,2)\n",
    "            plt.imshow(rgb_image)\n",
    "            plt.axis('off')\n",
    "            plt.show()\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pull out the date and code from save bad\n",
    "\n",
    "save_bad_codes = [fn.split('_')[0] + '_' + fn.split('_')[1] for fn in save_bad]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#moving raw subdirectories for bad PS images to a different directory\n",
    "path = f'/data0/images/planet/emma/planet/{name}/'\n",
    "move_to = f'/data0/images/planet/emma/planet/{name}/not_good/'\n",
    "dir = glob.glob(path + '/*.tif')\n",
    "save_bad_codes = [fn.split('_')[0] + '_' + fn.split('_')[1] for fn in save_bad]\n",
    "\n",
    "for file in ps_raw_subdir:\n",
    "    fn = os.path.basename(file)\n",
    "    date = fn.split('_')[0]\n",
    "    id = fn.split('_')[1]\n",
    "    code = f'{date}_{id}'\n",
    "    subdir = file.split('/')[-1]\n",
    "    if code in save_bad_codes:\n",
    "        src = path+subdir\n",
    "        dst = move_to+subdir\n",
    "        shutil.move(src, dst)\n",
    "        # print(subdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move processed bad files to a new directory\n",
    "other_model = 'V9'\n",
    "path = f'/data0/images/planet/emma/planet/processed_SCA/{name}/{other_model}/'\n",
    "move_to = f'/data0/images/planet/emma/planet/processed_SCA/{name}/{other_model}/not_good/'\n",
    "dir = glob.glob(path + '/*.tif')\n",
    "for file in dir:\n",
    "    if os.path.basename(file) in save_bad:\n",
    "        print(file)\n",
    "        src = path+os.path.basename(file)\n",
    "        dst = move_to+os.path.basename(file)\n",
    "        shutil.move(src, dst)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "\n",
    "# Get list of .tif files from both directories\n",
    "dir_v5 = glob.glob('/data0/images/planet/emma/planet/processed_SCA/BUDD/V5/*.tif')\n",
    "dir_v9 = glob.glob('/data0/images/planet/emma/planet/processed_SCA/BUDD/V9/*.tif')\n",
    "\n",
    "# Get just the basenames (filename without the full path)\n",
    "basenames_v5 = set(os.path.basename(f) for f in dir_v5)\n",
    "basenames_v9 = set(os.path.basename(f) for f in dir_v9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find files that are in V5 but not in V9\n",
    "files_in_v5_only = basenames_v5 - basenames_v9\n",
    "files_in_v5_only\n",
    "\n",
    "# Find files that are in V9 but not in V5\n",
    "files_in_v9_only = basenames_v9 - basenames_v5\n",
    "files_in_v9_only\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_in_v5_only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import netCDF4\n",
    "import xarray as xr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file = '/home/etboud/projects/snow_mapping/Emma/BUDD_V5_2022_3_10_NDVI_QAQC.nc'\n",
    "file = '/home/etboud/projects/snow_mapping/Emma/DLNY_V5_2023_10_10_NDVI_QAQC.nc'\n",
    "dataset = xr.open_dataset(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(glob.glob('/data0/images/planet/emma/planet/processed_SCA/BUDD/V5' + '/*2022*.tif')))\n",
    "print(len(glob.glob('/data0/images/planet/emma/planet/processed_SCA/BUDD/V9' + '/*2022*.tif')))\n",
    "print(len(glob.glob('/data0/images/planet/emma/planet/processed_SCA/BUDD/V7' + '/*2022*.tif')))\n",
    "print(len(glob.glob('/data0/images/planet/emma/planet/processed_SCA/DLNY/V5' + '/*2023*.tif')))\n",
    "print(len(glob.glob('/data0/images/planet/emma/planet/processed_SCA/DLNY/V9' + '/*2023*.tif')))\n",
    "print(len(glob.glob('/data0/images/planet/emma/planet/processed_SCA/DLNY/V7' + '/*2023*.tif')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir.sort()\n",
    "for file in dir:\n",
    "    if os.path.basename(file)[:4] == year:\n",
    "        print(os.path.basename(file))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visually check single file\n",
    "#file = '/home/etboud/projects/data/planet/processed_SCA/LYMC/V1/20230628_174540_SCA.tif'\n",
    "#file = '/home/etboud/projects/data/planet/processed_SCA/LYMC/V2/20230628_174540_SCA.tif'\n",
    "# file = '/home/etboud/projects/data/planet/processed_SCA/LYMC/V2/20230622_182552_SCA.tif'\n",
    "# sca = rioxarray.open_rasterio(file, all_touched=False, drop=True, masked=True)\n",
    "# sca.values = np.where(np.isnan(sca.values), 0, sca.values)\n",
    "# sca = sca.rio.clip(basin.geometry.values, crs=basin.crs, drop=True)\n",
    "# sca = np.squeeze(sca.values)\n",
    "# im = plt.imshow(sca,vmin= 0,vmax = 2, interpolation= 'none')\n",
    "# plt.colorbar(im)\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "# # Visually check aso validation tifs\n",
    "# for file in val_tif:\n",
    "#     aso = rioxarray.open_rasterio(file, all_touched=False, drop=True, masked=True)\n",
    "#     aso.values = np.where(np.isnan(aso.values), 0, aso.values)\n",
    "#     aso = aso.rio.clip(basin.geometry.values, crs=basin.crs, drop=True)\n",
    "#     aso = np.squeeze(aso.values)\n",
    "#     im = plt.imshow(aso,vmin= 0,vmax = 2, interpolation= 'none')\n",
    "#     plt.colorbar(im)\n",
    "#     plt.show()\n",
    "\n",
    "for file in ps_sca_tif:\n",
    "    sca = rioxarray.open_rasterio(file, all_touched=False, drop=True, masked=True)\n",
    "    sca.values = np.where(np.isnan(sca.values), 0, sca.values)\n",
    "    sca = sca.rio.clip(basin.geometry.values, crs=basin.crs, drop=True)\n",
    "    sca = np.squeeze(sca.values)\n",
    "    im = plt.imshow(sca,vmin= 0,vmax = 2, interpolation= 'none')\n",
    "    plt.colorbar(im)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Corresponding Files in csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def closest_dates(ps_dates, aso_date):\n",
    "    # Extract dates from tuples\n",
    "    ps_date_list = [date for date, _ in ps_dates]\n",
    "    # Sort the ps_dates\n",
    "    sorted_list = sorted(ps_date_list)\n",
    "    \n",
    "    closest_distance = None\n",
    "    closest_dates = []\n",
    "\n",
    "    for date in sorted_list:\n",
    "        distance = abs((date - aso_date).days)\n",
    "        \n",
    "        if closest_distance is None or distance < closest_distance:\n",
    "            closest_distance = distance\n",
    "            closest_dates = [date]\n",
    "        elif distance == closest_distance:\n",
    "            closest_dates.append(date)\n",
    "\n",
    "    return closest_dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# create dataframe to store all PS filenames with corresponding ASO flights\n",
    "raw_fn = []\n",
    "for aso_date, aso_file in sorted(aso_dates):\n",
    "    closest = closest_dates(ps_dates, aso_date)\n",
    "\n",
    "    closest_str = closest[0].strftime('%Y%m%d') # change index fromo to -1 to get the preceeding or following closest date when dates adjacent are equadistance\n",
    "    # ps_dates = pd.to_datetime(closest_str, format='%Y-%m-%d') \n",
    "    print(closest_str)\n",
    "    for fn in ps_sca_tif:\n",
    "        if closest_str in fn:\n",
    "            matching_model_files = glob.glob(fn)\n",
    "            code = fn.split('/')[-1].split('_')[1]\n",
    "            closest_code = f'{closest_str}_{code}'\n",
    "    for file in ps_raw_subdir:\n",
    "        if closest_code in file:\n",
    "            matching_raw_files = glob.glob(file+'/*/PSScene/*_SR_clip.tif')[0]\n",
    "            raw_fn.append({\"date\": closest_str, \"raw_file\": matching_raw_files,\"model_file\": matching_model_files, \"aso_file\": aso_file,\"aso_date\": aso_date})\n",
    "            \n",
    "pd_2020_0 = pd.DataFrame(raw_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>raw_file</th>\n",
       "      <th>model_file</th>\n",
       "      <th>aso_file</th>\n",
       "      <th>aso_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20220421</td>\n",
       "      <td>/data0/images/planet/emma/planet/COPP/20220421...</td>\n",
       "      <td>[/home/etboud/projects/data/temp/COPP/base/202...</td>\n",
       "      <td>/home/etboud/projects/data/aso/validation/COPP...</td>\n",
       "      <td>2022-04-21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20220515</td>\n",
       "      <td>/data0/images/planet/emma/planet/COPP/20220515...</td>\n",
       "      <td>[/home/etboud/projects/data/temp/COPP/base/202...</td>\n",
       "      <td>/home/etboud/projects/data/aso/validation/COPP...</td>\n",
       "      <td>2022-05-18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       date                                           raw_file  \\\n",
       "0  20220421  /data0/images/planet/emma/planet/COPP/20220421...   \n",
       "1  20220515  /data0/images/planet/emma/planet/COPP/20220515...   \n",
       "\n",
       "                                          model_file  \\\n",
       "0  [/home/etboud/projects/data/temp/COPP/base/202...   \n",
       "1  [/home/etboud/projects/data/temp/COPP/base/202...   \n",
       "\n",
       "                                            aso_file   aso_date  \n",
       "0  /home/etboud/projects/data/aso/validation/COPP... 2022-04-21  \n",
       "1  /home/etboud/projects/data/aso/validation/COPP... 2022-05-18  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd_2020_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = pd_2020_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = pd.concat([pd_2022_0,pd_2022_1,pd_2023_0,pd_2023_1], ignore_index=True) #run in different cell after creating multiple DF for each year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_string(x):\n",
    "    if isinstance(x, list):\n",
    "        # Convert list to string, then clean it\n",
    "        x = ', '.join(map(str, x))\n",
    "    elif not isinstance(x, str):\n",
    "        return x  # Return as is if not a string or list\n",
    "\n",
    "    # Now x is guaranteed to be a string, so we can clean it\n",
    "    return x.replace('[', '').replace(']', '').replace(\"'\", '')\n",
    "\n",
    "# Apply the cleaning function to both columns\n",
    "merged['raw_file'] = merged['raw_file'].apply(clean_string)\n",
    "merged['model_file'] = merged['model_file'].apply(clean_string)\n",
    "\n",
    "# Format date columns\n",
    "merged['ps_date'] = pd.to_datetime(merged['date']).dt.strftime('%Y-%m-%d')\n",
    "merged['aso_date'] = pd.to_datetime(merged['aso_date']).dt.strftime('%Y-%m-%d')\n",
    "\n",
    "# Drop the original date column\n",
    "merged.drop(columns=['date'], inplace=True)\n",
    "merged_drop = merged.drop_duplicates()\n",
    "#merged_drop.to_csv('corresponding_files.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_drop.to_csv('COPP_corresponding_files.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "existing = pd.read_csv('/home/etboud/projects/ps_sca/corresponding_files.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "existing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([existing, merged_drop], ignore_index=True).to_csv('corresponding_files_all.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through the corresponding files\n",
    "for index, row in merged_drop.iterrows():\n",
    "    raw_fn = row['raw_file']\n",
    "    model_fn = row['model_file']\n",
    "    aso_fn = row['aso_file']\n",
    "    \n",
    "    fn = raw_fn.split('/')[-1].split('_')\n",
    "    id = f'{fn[0]}_{fn[1]}'\n",
    "    \n",
    "    rgb_image = rxr.open_rasterio(raw_fn,masked = True, drop=True)\n",
    "    rgb_image = rgb_image.rio.clip(basin.geometry, basin.crs)\n",
    "    _, _, _, _, rgb_image = calc_rgb(rgb_image)\n",
    "\n",
    "    \n",
    "    model_image = rxr.open_rasterio(model_fn, masked=True)\n",
    "    model_image.values = np.where(np.isnan(model_image.values), 0, model_image.values)\n",
    "    model_image = model_image.rio.clip(basin.geometry, basin.crs)\n",
    "    \n",
    "    aso = rxr.open_rasterio(aso_fn, masked=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating netcdf of processed snow maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processed snow maps\n",
    "p_dir = f'/data0/images/planet/emma/planet/rerun/BUDD/V5/'\n",
    "p_tifs = glob.glob(p_dir + '*.tif')\n",
    "# Post processed snow maps\n",
    "pp_nc = '/data0/images/planet/emma/planet/pp_rerun/BUDD_V5_2023_10_10_NDVI_QAQC.nc'\n",
    "pp = xr.open_dataset(pp_nc)\n",
    "\n",
    "PP_times = pp.time.values\n",
    "PP_times\n",
    "\n",
    "dates_list = []\n",
    "data_list = []\n",
    "for fcount, file in enumerate(sorted(p_tifs)):\n",
    "    date = os.path.basename(file).split('_')[0]\n",
    "    date = datetime.strptime(date, '%Y%m%d')\n",
    "    date = np.datetime64(date)\n",
    "    \n",
    "    if date in PP_times:\n",
    "        print(date)\n",
    "        dates_list.append(date)\n",
    "        \n",
    "        data = rxr.open_rasterio(file, drop = True, masked = True)\n",
    "        data.values = np.where(0,np.isnan(data.values), data.values)\n",
    "        clipped_data = data.rio.clip(basin.geometry.values, basin.crs)\n",
    "        clipped_data = clipped_data.rio.clip(rgi_mask.geometry.values, invert= True)\n",
    "        clipped_data = clipped_data.rio.clip(wbd_mask.geometry.values, invert= True)\n",
    "        data_list.append(clipped_data.squeeze().values)\n",
    "dates = np.array(dates_list)\n",
    "dates = pd.to_datetime(dates, format='%Y%m%d')\n",
    "stacked_data = np.stack(data_list, axis = 0).astype(float)\n",
    "xr_data = xr.DataArray(stacked_data, dims=['time', 'y', 'x'], coords={'time': dates, 'y': np.arange(stacked_data.shape[1]), 'x': np.arange(stacked_data.shape[2])})\n",
    "xr_data.to_netcdf(f'BUDD_V5_2023_model_output.nc',format='NETCDF4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'BUDD'\n",
    "model = 'V5'\n",
    "year = '2020'\n",
    "# Basin shapefile\n",
    "BS = f'/home/etboud/projects/data/basins/{name}/{name}_4326.geojson'\n",
    "basin = gpd.read_file(BS)\n",
    "basin = basin.to_crs('EPSG:32611') \n",
    "\n",
    "rgi = '/home/etboud/projects/data/RGI/02_rgi60_WesternCanadaUS/02_rgi60_WesternCanadaUS.shp'\n",
    "rgi_mask = gpd.read_file(rgi).to_crs('EPSG:32611')\n",
    "wbd = '/home/etboud/projects/data/masks/NHDWaterbody.shp'\n",
    "wbd_mask = gpd.read_file(wbd).to_crs('EPSG:32611')\n",
    "# Raw planet images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processed snow maps\n",
    "# p_dir = f'/data0/images/planet/emma/planet/rerun/BUDD/V5/'\n",
    "p_dir = '/home/etboud/projects/data/temp/BUDD/V9/'\n",
    "p_tifs = glob.glob(p_dir + '*.tif')\n",
    "# Post processed snow maps\n",
    "pp_nc = '/home/etboud/projects/snow_mapping/Emma/BUDD_V5PP_2020_3_50_NDVI_QAQC_2.nc'\n",
    "pp = xr.open_dataset(pp_nc)\n",
    "\n",
    "PP_times = pp.time.values\n",
    "PP_times\n",
    "\n",
    "dates_list = []\n",
    "data_list = []\n",
    "for fcount, file in enumerate(sorted(p_tifs)):\n",
    "    date = os.path.basename(file).split('_')[0]\n",
    "    date = datetime.strptime(date, '%Y%m%d')\n",
    "    date = np.datetime64(date)\n",
    "    \n",
    "    if date in PP_times:\n",
    "        print(date)\n",
    "        dates_list.append(date)\n",
    "        \n",
    "        data = rxr.open_rasterio(file, drop = True, masked = True)\n",
    "        data.values = np.where(0,np.isnan(data.values), data.values)\n",
    "        clipped_data = data.rio.clip(basin.geometry.values, basin.crs)\n",
    "        clipped_data = clipped_data.rio.clip(rgi_mask.geometry.values, invert= True)\n",
    "        clipped_data = clipped_data.rio.clip(wbd_mask.geometry.values, invert= True)\n",
    "        data_list.append(clipped_data.squeeze().values)\n",
    "dates = np.array(dates_list)\n",
    "dates = pd.to_datetime(dates, format='%Y%m%d')\n",
    "stacked_data = np.stack(data_list, axis = 0).astype(float)\n",
    "xr_data = xr.DataArray(stacked_data, dims=['time', 'y', 'x'], coords={'time': dates, 'y': np.arange(stacked_data.shape[1]), 'x': np.arange(stacked_data.shape[2])})\n",
    "xr_data.to_netcdf(f'BUDD_V_2020_model_output.nc',format='NETCDF4')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
